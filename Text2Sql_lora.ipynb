{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d95836-777a-410e-b2e6-2a2f6e74f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"tinyllama_Text2Sql_lora\"\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer, AutoTokenizer, TrainingArguments, set_seed\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "import re\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949f162e-fafb-4685-bea7-74610802dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89575750-bcfb-4fcc-b3f4-ad559bb2fed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad22f51db1942ad92a4893dd3c78d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea5da3ca42a406486f27f2963d86b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0421b079d5410bbd83af02291ff975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d425a5104a4e4fa89c7253e3711bbec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb12884-ed5b-49c0-b107-a4e5a979b781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3106f36e55b46fa83de529c3cffd0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964d6cf826be4305ad966bc910bc8509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)nthetic_text_to_sql_train.snappy.parquet:   0%|          | 0.00/32.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba033ca986443c3b7d76406d55304fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ynthetic_text_to_sql_test.snappy.parquet:   0%|          | 0.00/1.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460b5e4cd18e4b778981cc939928eab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bf9e59a48d4be2ac3405bbded8cb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"gretelai/synthetic_text_to_sql\"\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acecb9ad-52eb-479a-902f-0bcba58871e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "        num_rows: 5851\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb49660-7b36-4b1f-ac0e-bc5a06a63965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_schema(entry):\n",
    "#     schema = ''\n",
    "#     for stmt in entry['sql_context'].split(';'):\n",
    "#         if 'CREATE TABLE' in stmt:\n",
    "#             print(stmt)\n",
    "#             table_name = stmt.split()[2]\n",
    "#             columns = stmt.split('(', 1)[1].rsplit(')', 1)[0]\n",
    "#             col_defs = columns.split(',')\n",
    "#             schema = schema + \"Table: \" + table_name + \"\\n\"\n",
    "#             for col in col_defs:\n",
    "#                 parts = col.strip().split()\n",
    "#                 if len(parts)>= 2:\n",
    "#                     schema = schema + \"- \" + parts[0] + \": \" + parts[1].upper()\n",
    "#             schema = schema + \"\\n\"\n",
    "#     return {\"schema\": schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466599ff-cc6d-403b-a3c3-3a319d0d88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def get_schema(entry):\n",
    "#     schema = ''\n",
    "#     for stmt in entry['sql_context'].split(';'):\n",
    "#         stmt = stmt.strip()\n",
    "#         if stmt.upper().startswith('CREATE TABLE') and 'AS SELECT' in stmt.upper():\n",
    "#             # Try to extract table name\n",
    "#             table_name = stmt.split()[2]\n",
    "#             select_part = stmt.upper().split('AS SELECT', 1)[1]\n",
    "#             # Extract columns before FROM\n",
    "#             columns_raw = re.split(r'\\bFROM\\b', select_part, 1)[0]\n",
    "#             columns = [c.strip().split()[-1] for c in columns_raw.split(',')]\n",
    "#             schema += \"Table: \" + table_name + \"\\n\"\n",
    "#             for col in columns:\n",
    "#                 schema += f\"- {col}: UNKNOWN\\n\"\n",
    "#         elif stmt.upper().startswith('CREATE TABLE'):\n",
    "#             table_name = stmt.split()[2]\n",
    "#             columns = stmt.split('(', 1)[1].rsplit(')', 1)[0]\n",
    "#             col_defs = columns.split(',')\n",
    "#             schema += \"Table: \" + table_name + \"\\n\"\n",
    "#             for col in col_defs:\n",
    "#                 parts = col.strip().split()\n",
    "#                 if len(parts) >= 2:\n",
    "#                     schema += \"- \" + parts[0] + \": \" + parts[1].upper() + \"\\n\"\n",
    "#     return {\"schema\": schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273f3dc0-b343-45d1-9910-97bf2f53eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(entry):\n",
    "    schema = ''\n",
    "    for stmt in entry['sql_context'].split(';'):\n",
    "        stmt = stmt.strip()\n",
    "        # Skip empty or invalid CREATE TABLEs\n",
    "        if not stmt.upper().startswith('CREATE TABLE'):\n",
    "            continue\n",
    "\n",
    "        # Try to extract the table name safely\n",
    "        try:\n",
    "            tokens = stmt.split()\n",
    "            table_name = tokens[2].strip(\"'`\\\"\")  # Strip extra quotes\n",
    "        except IndexError:\n",
    "            schema += \"Malformed CREATE TABLE statement skipped.\\n\"\n",
    "            continue\n",
    "\n",
    "        # Handle CTAS\n",
    "        if 'AS SELECT' in stmt.upper():\n",
    "            select_part = stmt.upper().split('AS SELECT', 1)[1]\n",
    "            columns_raw = re.split(r'\\bFROM\\b', select_part, 1)[0]\n",
    "            columns = [c.strip().split()[-1] for c in columns_raw.split(',')]\n",
    "            schema += f\"Table: {table_name}\\n\"\n",
    "            for col in columns:\n",
    "                schema += f\"- {col}: UNKNOWN\\n\"\n",
    "        # Handle regular CREATE TABLE with column definitions\n",
    "        elif '(' in stmt and ')' in stmt:\n",
    "            try:\n",
    "                columns = stmt.split('(', 1)[1].rsplit(')', 1)[0]\n",
    "                col_defs = columns.split(',')\n",
    "                schema += f\"Table: {table_name}\\n\"\n",
    "                for col in col_defs:\n",
    "                    parts = col.strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        schema += f\"- {parts[0]}: {parts[1].upper()}\\n\"\n",
    "            except Exception:\n",
    "                schema += f\"Table: {table_name} (column parsing error)\\n\"\n",
    "        else:\n",
    "            # Malformed table definition (no columns or SELECT)\n",
    "            schema += f\"Table: {table_name}\\n\"\n",
    "\n",
    "    return {\"schema\": schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7901c1e7-db4b-4f61-a27f-9fdf21215719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d253220c8ae64a179f02506c7099dd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52282dff173348528fec0151bba499ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].map(get_schema)\n",
    "dataset['test'] = dataset['test'].map(get_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7302165-b002-4f57-a1be-b23fdd0fcab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation', 'schema'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation', 'schema'],\n",
       "        num_rows: 5851\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c374851-070c-48af-b6db-33b358cbe579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "template_str = \"\"\"\n",
    "<|im_start|>system\n",
    "You are a SQL assistant. Use the following schema to answer queries.\n",
    "{{ schema.strip() }}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{{ sql_prompt }}\n",
    "<|im_end|>\n",
    "{% if sql is defined and is_training == True %}\n",
    "<|im_start|>assistant\n",
    "{{ sql }}\n",
    "<|im_end|>\n",
    "{% endif %}\n",
    "{% if add_generation_prompt %}\n",
    "<|im_start|>assistant\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "\n",
    "jinja_template = Template(template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1690d0d-659f-4dba-b455-15aac38c2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples, is_training=True, add_generation_prompt=False):\n",
    "    contents = []\n",
    "    for i in range(len(examples['sql_prompt'])):\n",
    "        rendered = jinja_template.render(\n",
    "            schema = examples['schema'][i],\n",
    "            sql_prompt=examples['sql_prompt'][i],\n",
    "            context=examples['sql_context'][i],\n",
    "            sql=examples['sql'][i] if is_training and 'sql' in examples else None,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            is_training = is_training\n",
    "        ).strip()\n",
    "        contents.append(rendered)\n",
    "    return {'messages': contents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3e488c-2707-4c39-b137-537fa27fa5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086d92c1e1454210a7b13e8f3587f9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb9b5b11fd04dd9a565443f5060e02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].map(\n",
    "    lambda x: preprocess(x, is_training=True, add_generation_prompt=False),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "dataset['test'] = dataset['test'].map(\n",
    "    lambda x: preprocess(x, is_training=False, add_generation_prompt=True),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"test\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "438f91bd-5ea1-4355-813e-7f5fda3d688e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 5851\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9349800-dfd1-4d58-8460-5ac6c5aeccea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': '<|im_start|>system\\nYou are a SQL assistant. Use the following schema to answer queries.\\nTable: salesperson\\n- salesperson_id: INT\\n- name: TEXT\\n- region: TEXT\\nTable: timber_sales\\n- sales_id: INT\\n- salesperson_id: INT\\n- volume: REAL\\n- sale_date: DATE\\n<|im_end|>\\n<|im_start|>user\\nWhat is the total volume of timber sold by each salesperson, sorted by salesperson?\\n<|im_end|>\\n\\n<|im_start|>assistant\\nSELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\\n<|im_end|>'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4e30d89-3e41-48fd-825f-2ac6dbe5a43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': \"<|im_start|>system\\nYou are a SQL assistant. Use the following schema to answer queries.\\nTable: creative_ai\\n- application_id: INT\\n- name: TEXT\\n- region: TEXT\\n- explainability_score: FLOAT\\n<|im_end|>\\n<|im_start|>user\\nWhat is the average explainability score of creative AI applications in 'Europe' and 'North America' in the 'creative_ai' table?\\n<|im_end|>\\n\\n\\n<|im_start|>assistant\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "074e2b81-0fc0-4973-91c1-60ba5fd2b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{% for message in messages %}\\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\\n' }}{% endif %}{% endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79013991-e248-44bd-b030-16808354de4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa0b638f20044d494c11e0524461de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29839979d084bd7a679f89290180b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db79e9f091e4f07bcff77c6b9196811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32005, 2048)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    user = \"<|im_start|>user\"\n",
    "    assistant = \"<|im_start|>assistant\"\n",
    "    system = \"<|im_start|>system\"\n",
    "    eos_token = \"<|im_end|>\"\n",
    "    bos_token = \"<s>\"\n",
    "    pad_token = \"<pad>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "        bos_token=ChatmlSpecialTokens.bos_token.value,\n",
    "        eos_token=ChatmlSpecialTokens.eos_token.value,\n",
    "        additional_special_tokens=ChatmlSpecialTokens.list(),\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "tokenizer.chat_template = template\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3fb9b69-7dc5-446f-826a-580717b7399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side=\"left\"\n",
    "def get_prediction_batched(samples, column_name):\n",
    "    # batch = []\n",
    "    # for conversation in samples[\"messages\"]:\n",
    "    #     chatml_gen_prompt = tokenizer.apply_chat_template(conversation[:-1], tokenize=False, add_generation_prompt=True)\n",
    "    #     batch.append(chatml_gen_prompt)\n",
    "    #text = tokenizer.apply_chat_template(conversation_history, add_generation_prompt=True, tokenize=False)\n",
    "    inputs = tokenizer(samples[\"messages\"], return_tensors=\"pt\", padding=True, truncation=True)#, add_special_tokens=False)\n",
    "    inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=100, \n",
    "                             do_sample=True, \n",
    "                             top_p=0.95, \n",
    "                             temperature=0.2, \n",
    "                             repetition_penalty=1.1, \n",
    "                             eos_token_id=tokenizer.eos_token_id,\n",
    "                             pad_token_id=tokenizer.eos_token_id,\n",
    "                            )\n",
    "    outputs = tokenizer.batch_decode(outputs)\n",
    "    outputs = [output.split(\"<|im_start|>assistant\")[-1].split(\"<|im_end|>\")[0].strip() for output in outputs]\n",
    "    return {column_name: outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b1a880b-4c32-4862-9d49-626abe514ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4945598eaea40b09a7b3e57e90a6134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "test_dataset = dataset[\"test\"].shuffle().select(range(25))\n",
    "test_dataset = test_dataset.map(\n",
    "    partial(get_prediction_batched, column_name=\"base_assistant_message\"),\n",
    "    batched=True,\n",
    "    batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "557dce13-48dd-4ded-9d09-6852884284a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(r=8,\n",
    "                         lora_alpha=16,\n",
    "                         lora_dropout=0.1,\n",
    "                         target_modules=[\"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"],\n",
    "                         task_type=TaskType.CAUSAL_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "734311cb-b523-4a32-aaf4-ae5575515d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32005, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32005, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a060495f-4fbb-4be2-be58-08cd26b79eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,852,688 || all params: 1,106,921,552 || trainable%: 0.6191\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# cast non-trainable params in fp16\n",
    "for p in model.parameters():\n",
    "    if not p.requires_grad:\n",
    "        p.data = p.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3179456e-f0bd-4694-89f8-9d579f18bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"messages\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b7c7007-edf6-4144-b6df-25205e498381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5851\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b1b6193-d994-4199-8900-29f46bb06692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"tinyllama_Text2Sql_lora\"\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 1\n",
    "gradient_accumulation_steps = 16\n",
    "logging_steps = 25\n",
    "learning_rate = 2e-5\n",
    "max_grad_norm = 1.0\n",
    "max_steps = 250\n",
    "num_train_epochs=2\n",
    "warmup_ratio = 0.1\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    fp16=True,\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    hub_private_repo=True,\n",
    "    push_to_hub=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e476c295-8458-477d-b390-63ed3120d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].shuffle().select(range(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb94dbb3-bb8f-492e-9744-5e9933cf35a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_901/3239419570.py:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea414797b7841b389d1ea1840117b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f75b1889ab4f7e9b6ccf6936d0bcda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7cadd4595d463db04a2364d6ba81df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a8fbf446694e15a47243c23908dd1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9d3f9ce19943b5b97135a561d59388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/5851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a956fe26c2413bb2405a845db3eedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/5851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e49283895c9402eb0f955335663e622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/5851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854370d4b2654d2fa534d29686d30126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/5851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    # packing=True,\n",
    "    # dataset_text_field=\"content\",\n",
    "    # max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8062a57-b753-462d-af26-67672b04262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprojectsbyswathi\u001b[0m (\u001b[33mprojectsbyswathi-na\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250506_101110-t3gcgbhn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/projectsbyswathi-na/tinyllama_Text2Sql_lora/runs/t3gcgbhn' target=\"_blank\">tinyllama_Text2Sql_lora</a></strong> to <a href='https://wandb.ai/projectsbyswathi-na/tinyllama_Text2Sql_lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/projectsbyswathi-na/tinyllama_Text2Sql_lora' target=\"_blank\">https://wandb.ai/projectsbyswathi-na/tinyllama_Text2Sql_lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/projectsbyswathi-na/tinyllama_Text2Sql_lora/runs/t3gcgbhn' target=\"_blank\">https://wandb.ai/projectsbyswathi-na/tinyllama_Text2Sql_lora/runs/t3gcgbhn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='624' max='624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [624/624 49:58, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.763800</td>\n",
       "      <td>1.029207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b00fd9ac-1077-4a1d-a184-78afb975c667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279019ebd72641d789b534381d44ae60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954b72c8e95947e8ae42f34b3965a0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5932216f2b794a55b5ef5cc1889c6e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab69b8a452642848bb419f780b8b0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce5efce46ad4951be1c83ce828c5a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/488 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): lora.Embedding(\n",
       "          (base_layer): Embedding(32005, 2048)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 8x32005 (cuda:0)])\n",
       "          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 2048x8 (cuda:0)])\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): lora.Linear(\n",
       "        (base_layer): Linear(in_features=2048, out_features=32005, bias=False)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=32005, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and match special tokens\n",
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    user = \"<|im_start|>user\"\n",
    "    assistant = \"<|im_start|>assistant\"\n",
    "    system = \"<|im_start|>system\"\n",
    "    eos_token = \"<|im_end|>\"\n",
    "    bos_token = \"<s>\"\n",
    "    pad_token = \"<pad>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Swathi8378/tinyllama_Text2Sql_lora\",\n",
    "    pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "    bos_token=ChatmlSpecialTokens.bos_token.value,\n",
    "    eos_token=ChatmlSpecialTokens.eos_token.value,\n",
    "    additional_special_tokens=ChatmlSpecialTokens.list(),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.chat_template = template  # Set this if required\n",
    "\n",
    "# Load base model FIRST (same architecture)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T\",  # or the correct base\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Resize embeddings to match tokenizer (must be done BEFORE loading LoRA)\n",
    "base_model.resize_token_embeddings(len(tokenizer))  # Ensure shape [32005, hidden_size]\n",
    "\n",
    "# Now load the PEFT adapter\n",
    "from peft import PeftModel\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"Swathi8378/tinyllama_Text2Sql_lora\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eec88ad4-e972-41b4-bb0f-896d405c3f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57b04f0ced64fe4863b31bcdf805316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = test_dataset.map(\n",
    "    partial(get_prediction_batched, column_name=\"instruct_assistant_message\"),\n",
    "    batched=True,\n",
    "    batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a17e7658-4250-4953-b302-700f36abea50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>base_assistant_message</th>\n",
       "      <th>instruct_assistant_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a SQL assistant. U...</td>\n",
       "      <td>What is the average maintenance cost for all m...</td>\n",
       "      <td>SELECT name, maintenance_cost FROM MilitaryEqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a SQL assistant. U...</td>\n",
       "      <td></td>\n",
       "      <td>INSERT INTO volunteers (organization_id, name)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages  \\\n",
       "0  <|im_start|>system\\nYou are a SQL assistant. U...   \n",
       "1  <|im_start|>system\\nYou are a SQL assistant. U...   \n",
       "\n",
       "                              base_assistant_message  \\\n",
       "0  What is the average maintenance cost for all m...   \n",
       "1                                                      \n",
       "\n",
       "                          instruct_assistant_message  \n",
       "0  SELECT name, maintenance_cost FROM MilitaryEqu...  \n",
       "1  INSERT INTO volunteers (organization_id, name)...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = test_dataset.to_pandas()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7fe64b3-433a-443c-a1b3-2929ee2f034a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a SQL assistant. Use the following schema to answer queries.\\nTable: MilitaryEquipment\\n- equipment_id: INT\\n- name: VARCHAR(255)\\n- region: VARCHAR(255)\\n- maintenance_cost: FLOAT\\n<|im_end|>\\n<|im_start|>user\\nWhat are the names and maintenance costs of all military equipment in the Atlantic region with a maintenance cost less than $5000?\\n<|im_end|>\\n\\n\\n<|im_start|>assistant'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['messages'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a92b90f3-33af-42b6-b0a8-a2f49a6eab10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the average maintenance cost for all military equipment in the Atlantic region?\\nWhat is the average maintenance cost for all military equipment in the Pacific region?\\nWhat is the average maintenance cost for all military equipment in the Indian Ocean region?\\nWhat is the average maintenance cost for all military equipment in the Mediterranean region?\\nWhat is the average maintenance cost for all military equipment in the North American region?\\nWhat is the average maintenance cost for all military equipment in the South American'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['base_assistant_message'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03cd3f38-1760-47bf-ae8a-09ba7584f8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SELECT name, maintenance_cost FROM MilitaryEquipment WHERE region = 'Atlantic' AND maintenance_cost < 5000;\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['instruct_assistant_message'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30cea8c7-7a4b-4016-bfbb-dcc6190ce9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are a SQL assistant. Use the following schema to answer queries.\\nTable: organizations\\n- id: INT\\n- name: TEXT\\nTable: volunteers\\n- id: INT\\n- organization_id: INT\\n- name: TEXT\\n<|im_end|>\\n<|im_start|>user\\nInsert new records for 3 additional volunteers for the 'Doctors Without Borders' organization.\\n<|im_end|>\\n\\n\\n<|im_start|>assistant\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['messages'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f397b-0a15-4bbf-88d8-0df61ab81e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
